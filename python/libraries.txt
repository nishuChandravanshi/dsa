SUDOKU SOLVER description

# part2 image processing (giving bernard eye)
#  https://medium.com/@neshpatel/solving-sudoku-part-ii-9a7019d196a2

#numbered the tasks we need to do

1. We need a method for identifying the grid in the center
# There are functions we can use to try to detect the corners in the image, but before doing that
 we should process the image to improve the reliability of those operations->


-for programming using images weve used: open source project, OpenCV
    python binding of openCV : cv2 library 
    import cv2 #and use

    # eg 
    import cv2
    img = cv2.imread('images/1-original.jpg', cv2.IMREAD_COLOR) #cv2.IMREAD_GRAYSCALE -> to read image as greyscale: 
                                                                                        to remove unnecessary colors
    print(type(img))
    print(img.shape)
    print(img)


-each pixel is represented by a tuple of 3 representing the Blue-Green-Red (BGR) colour channels (it defaults to BGR, not RGB). 
Each channel is an integer between 0 and 255, so (0, 0, 0) is pure black and (255, 255, 255) is pure white. 
These are in a list of lists (2-D matrix) where the number of rows is the height of the image and the number of columns the width.

-read the image as greyscale (use cv2.IMREAD_GRAYSCALE), 
the colour information is lost and each element simply becomes a single integer between 0 and 255, 
representing the brightness of the pixel.

-At the moment the image is mostly greys and each pixel as some grey value between 0 and 255
This is quite noisy for the operations we will perform for corner detection
    -a way to reduce this noise(ie making clarity out of greyscale img weve) is THRESHOLDING

Binary thresholding 
    -These algorithms reduce an image to pure black and white, based on the contrast
    1. Global thresholding: makes the split based on a threshold measured from the entire image
    2. Adaptive thresholding: calculates a threshold for each pixel in the image based on the mean value of surrounding pixels
        -This is useful when the contrast is uneven across parts of the image,
    #so we'll be using adaptive threshold as the contrast of image may not be even mostly (light effect n all)
    
    import cv2
    import numpy as np
    from matplotlib import pyplot as plt
    img = cv2.imread('images/1-original.jpg', cv2.IMREAD_GRAYSCALE) #grayscaling image
    
    ret, threshold1 = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)  #global threshold
    threshold2 = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2) #adaptive threshold
    
    plot_many_images([threshold1, threshold2], ['Global', 'Adaptive']) #will print the image returned after resp threshold


Blur and Dilation
    blur the image beforehand to reduce the noise picked up by the thresholding algorithm and
    dilate the image to increase the thickness of the line

    # Dilate the image to increase the size of the grid lines.
    kernel = np.array([[0., 1., 0.], [1., 1., 1.], [0., 1., 0.]])
    proc = cv2.dilate(proc, kernel) #dilate func used 

#now, we are assuming that sudoku is the largest single feature in the image

Contours
    -a way of describing the boundaries of a shape that has the exact same intensity
    -findContours (algorithm) function to detect the contours in the image 
    #Since we converted to binary tones using thresholding, we can use this func to detect boundaries now

    new_img, ext_contours, hier = cv2.findContours(processed.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) #external contour-> finds outerr boundary 
    new_img, contours, hier = cv2.findContours(processed.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE) #finds boundary of inner smaller areas as well

    problems in finding corners-->
    # The function cv2.contourArea is handy here as we can use it to easily get the largest feature in the image. 
    # We could use the Ramer-Douglas-Peucker algorithm to approximate the number of sides of the shape as well as 
     this would allow us to filter for rectangular objects only.
    #However during testing I found that this limitation gave false negatives when folds in the page gave the illusion that the grid had 5 sides instead of 4.

    @discuss
    #ie with this we aint getting the corners so now-> applying following logic to get four corners->
    
Logic to find corners
    Top left point has the smallest x and smallest y coordinate, so minimise x + y.
    Top right point has the largest x and the smallest y coordinate, so maximise x - y.
    Bottom right point has the largest x and the largest y coordinate, so maximise x + y.
    Bottom left point has the smallest x and the largest y coordinate, so minimise x - y.

    problem-->
    # The next step is to cut out the part of the image we need and throw away the garbage.
    #But the photo was taken at a slight angle, so whilst our points map out a rectangle, it’s not a perfect square.

    sol-->
    warpPerspective  #openCV function
    #warpPerspective used to make the perspective of img processed so far into a square
    #the warp transformation has made it look like we are viewing the board directly instead of at an angle

2. Identify the cells from the board
    -divide up the grid into 81 evenly sized squares, since we have already warped it into a square
    
    **** code snippet-->
        def infer_grid(img):
        """Infers 81 cell grid from a square image."""
        squares = []
        side = img.shape[:1]
        side = side[0] / 9
        for i in range(9):
            for j in range(9):
                p1 = (i * side, j * side)  # Top left corner of a bounding box
                p2 = ((i + 1) * side, (j + 1) * side)  # Bottom right corner of bounding box
                squares.append((p1, p2))
        return squares

    original = cv2.imread('images/1-original.jpg', cv2.IMREAD_GRAYSCALE)
    processed = pre_process_image(original) 
    corners = find_corners_of_largest_polygon(processed)
    cropped = crop_and_warp(original, corners)
    squares = infer_grid(cropped)

    # See https://gist.github.com/mineshpatel1/22e86200eee86ebe3e221343b26fc3f3#file-display_rects-py
    display_rects(cropped, squares)
    
    *****

    problem - 
        a lot of cells have overlapping grid lines and the digits aren’t very well centered
    sol - #(refer code and see logic)
        Taking advantage of the fact that Sudoku grids are always simple and can only contain single digits 
         in the middle of each box, 
        we can implement a function that finds the largest connected pixel structure found when searching 
         around the centre of each square.


//this is 10 class classification problem

#part3. neural programming (giving bernard brain)
#https://medium.com/@neshpatel/solving-sudoku-part-iii-3182fe747883

*classification model used: as we know the ip as well as the op

Neural Network Program 
    -It consists of layers of neurons, where each neuron is an object that holds a number between 0 and 1 called its activation.
     The output activations of each layer serves as part of the input for the neurons in the next layer 
     and so on until we reach the output.
    -the first layer of neurons is the input layer, providing the stimulus for the network. 
     The final layer is the output layer, with a neuron for each possible outcome that can be achieved. 
     The hidden layers are where the intelligence lies.
    -these hidden layers will pick up on different features in the input 
     and the combination of these features will indicate the right output
        For example, the number 9 could be represented as a loop near the top and a vertical line 
         and then a loop could be represented by 4 curves all placed adjacently. 
        The idea is then that different stimuli in the input layer will cause different 
         activations in each subsequent layer and eventually activate the correct neuron in the output layer.
    
    -Each connection between the neurons has a weight associated with it, 
     which is a number that can be positive or negative, set arbitrarily to begin with
    -The activation of a neuron is a weighted sum of all of the activations of connected neurons in the previous layer
    - We also want a threshold below which the neuron is inactive, 
     we can do this by just subtracting a term which we call the "bias".

    There are many Variations of Neural network like->
        Convolutional Neural network - good for image recognition
        Long short-term memory network - good for speech recognition


    #28x28 pixels = 784 neurons each holding a greyscale value b/w 0-1 : 
     (but till now we've processed our img and in our image those neurons will be holding either 0(black) or 1(while) value)
    #this number inside the neuron is called its activation
    #this 784 neurons make up the first layer of our network
    #the activation for each neuron in the following layers is based 
     on the weighted sum of all the activations in the previous layer + bias
     then we compose this sum (weighted sum+bias) with other func like sigmoid or ReLU
    #the last layer is (op layer) consist of 10 neurons each consisting of a digit from 0-9
    #pixel->edges->patterns->digits
    



    #every edge b/w any two neuron in two layers is assigned some weight, 
        we calculate the weighted sum (to find any edge or pattern) 
         and this weighted sum helps to recoginize which all neurons in next layer will be active 

        Weighted sum = w1.a1 +w2.a2+.....(where w= edge weight, a = activation)
        # this weighted sum may corresspond to any digit! but we want it between 0-1
        so we use "sigmoid function"!
                        -in sigmoid func(see the curve) for very -ve val op will be around 0 
                         and for very positive op will be around 1
                        #sigmoid(a) = 1/(1+e^(-a))
        #nowadays sigmoid func aint used mch ->instead ReLU (rectified linear unit is in use) : as it was easier to train with reLU and gave good results
                        #ReLU(a) = max(0,a)
                        #it works as-> the neuron would be activated if it  passes some threshold otherwise not. 



        #now we dont want neurons on next layer to be active when weighted sum>0,
        lets say we want to be active when weighted sum >10 => we want sum "bias" for inactivity
        So, add sum numebr (say -10) into the weighted sum before putting it inside sigmoid func
    
        #Learning => which weights and biases minimizes a certain cost function 
        ie getting the computer to find the right set of weight and bias out of all the datasets, 
         and so become able to solve the problem in hand
        #cost function : take the output that the n/w gives and the output which we wanted and then-> 
             add up the squares of the differences between each componenet. 
             Doing this for all thousands of trainig examples and averaging the results gives the Total Cost of the Network.
            #now-> the thing we're looking for is the -ve Gradient of this cost function 
             which tells how we need to change all the weight and biases, so as to most efficiently decrease the cost.


        Back Propagation: 
            -an algorithm for determinig how a single trainig example would like to nudge the weights and biases,
             not just in terms of whether they should go up or down, but in terms of what relative proportion 
             to those changes cause the most rapid decrease to the cost.
            A true gradient descent step would involve doing this for all the thousands of trainig examples 
             and averaging the desired changes that we get-> but thats computationally slow. 
@dis->      So instead we randomly subdivide the data into mini batches and 
             compute each step wrt a mini batch, repeatedly going through all of these mini batches and 
             making thsese adjustments we'll converge towards a local minimum of the cost function.

            # Backpropagation is really one instance of a more general technique called "reverse mode differentiation" 
             to compute derivatives of functions represented in some kind of directed graph form.
             

        Feedforward 
            -Then the activation of each neuron is part of the input for each neuron in the next layer. 
            This passing of information sequentially through layers is called feedforward.

        Implementation part->
        
            TensorFlow library:  to do the heavy lifting for the neural network
                -Tensorflow works by first declaring a graph that describes your network 
                 and then runs sessions to perform training and prediction.
                -Tensors (from which the library gets its name) are a bit like matrices
                  (but with important differences) in that they have some shape and dimensionality. 
                 Our placeholder for x, the input has a shape of [None, 784]
                 The missing value will be filled in by the number of input images we choose, 
                  e.g. 81 for each cell of a Sudoku grid, each with 784 pixel values.
                 The same is true for y_, but the second parameter is 10 as that is the number of possible classifications, 0-9. 
                  The shapes of these tensors are important when defining the layers

                ***code snippet
                    x = tf.placeholder(tf.float32, shape=[None, 784])  # Placeholder for input
                    y_ = tf.placeholder(tf.float32, shape=[None, 10])  # Placeholder for true labels (used in training)
                    hidden_neurons = 16  # Number of neurons in the hidden layer, constant
                ***



            Architecture-
                We will be implementing the simple network that has one hidden layer in between the input and output layers
                #our input images from the Sudoku grid are 28x28 pixels big which gives us 784 neurons to play with, 
                 one for each pixel intensity in the image.

#Error Function(ie cost func) used -> cross entropy , (other eg->mean squared root func )
#sigmoid func



#todo-> neural n/w, error func,dataset(used on it), activation func, neural network, entropy, gradient descent,  
    numpy, pandas lib, matplotlib, opencv2, scikitlearn, 

    neural network
    activation function (types : relu,...)
    activation function
    cross function
    gradient descent (=>related to calculation)
    feedforward and backward propagation

    framework <-> (in neural network)
        1. tensor flow, ->(library- keras)
        2. pytorch
        
*digital technologies for-> tcs


    later-> 
    popular mchine learning models name and definition
        linear regression, logistic regression,classification,svm(support vector machine)


##for GUI -> can be implemented using python library PKinter

*******************************************************************************


@discuss 
NumPy
#https://www.tutorialspoint.com/numpy/numpy_quick_guide.htm

    -In Python we have lists that serve the purpose of arrays, but they are slow to process.
    -NumPy aims to provide an array object that is up to 50x faster that traditional Python lists.
    -The array object in NumPy is called ndarray, it provides a lot of supporting functions that make working with ndarray very easy.
    # -Arrays are very frequently used in data science, where speed and resources are very important.

    Operations using NumPy
    Using NumPy, a developer can perform the following operations −
    Mathematical and logical operations on arrays.
    Fourier transforms and routines for shape manipulation.
    Operations related to linear algebra. NumPy has in-built functions for linear algebra and random number generation.

    # NumPy is often used along with packages like SciPy (Scientific Python) and Mat−plotlib (plotting library). 
    # This combination is widely used as a replacement for MatLab, a popular platform for technical computing.

    Why is NumPy Faster Than Lists
        NumPy arrays are stored at one continuous place in memory unlike lists, so processes can access and manipulate them very efficiently
        This behavior is called locality of reference in computer science.
        This is the main reason why NumPy is faster than lists. Also it is optimized to work with latest CPU architectures




    Which Language is NumPy written in?
        NumPy is a Python library and is written partially in Python, but most of the parts that require fast computation are written in C or C++.

# The source code for NumPy is located at this github repository https://github.com/numpy/numpy


matplotlib

pyplot



******************************************************************************
#gpu (graphic processing unit)


#Pixel
# https://www.webopedia.com/TERM/P/pixel.html

#data set
    1. training dataset: 
        -trainig dataset and testing dataset are maintained seperately so as to check model on other dataset than that on which it is trained inorder to check its accuracy when new problem is encountered
    
    2. testing dataset: 
    3. validation dataset: 
    // overfitting(def pdh) 
        

**************************************
Study resources

Neural Networking 
    book : http://neuralnetworksanddeeplearning.com/
    videos: https://www.youtube.com/watch?v=aircAruvnKk




******************************
regression
    - value prediction
    -continuous range me kch value chiye jb
    eg -> mark preduction, 

classification
    -certain class me classify krna which class it belongs to

clustering
    -beforehand we dk which classes


supervised learning
    - ans known
unsupervised
    -not preveously known the output
    eg ->divide similar minded people and others in class


models
    i. linear regression
        -given some values, to find some other values with the help of given
        eg x1,x2 given x3=? ==> draw line with the help of x1,x2 find x3

        but not all problems are linear --> can use quadratic, cubic, etc variables to train
        y = p1^x1+p2^x2..... (generally till 4 degree polynomial is used)
        -> this is how different curves are made and value of these p1,p2 ...is obtained by trainig model 


    ii. logistic regression: 
        - used in classification
        - tells the probability of whether true or false 
        (sigmoid function used)

    iii. svm (support vector machine) 
        (classification k liye, but regression can also be solved)
        https://towardsdatascience.com/https-medium-com-pupalerushikesh-svm-f4b42800e989
    

    iv. random forest model 
        -very good model
        (both classification and predict!) 
        -breaks discrete values -> 


        