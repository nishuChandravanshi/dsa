FIRST MAKE THE DOTS, THEN START PLOTTING THE MAP!
-1.last page of the book->quiz/question
-2. bolds
-3. beginning -> first para->last para.
-4.start now.
****
USER -> USER PROGRAM -> COMMAND INTERPRETER(eg shell) -> OS <-> HW (hardware)
    
    -user does not directly interact with os, there's an interface called COMMAND INTERPRETER
    -COMMAND INTERPRETER is a program that interprets the cmd of user.
    there are 2 types of cmd interpreter based OS
        1. text based os - DOS, UNIX
        2. GUI based os - windows, MAC
 
*main goal of os is user's CONVINIENCE. secondary is efficiency
*windows os is more convinient but unix os is more efficient

throughput 
    -no. of instructions performed per unit time

************
PROCESS
#http://courses.cs.vt.edu/csonline/OS/Lessons/Processes/index.html

    program- 
        static set of directions or instructions
    process - 
        the dynamic activity whose properties change as time progresses

    -A process is an instance of program in execution
    For example a Web Browser is a process, a shell (or command prompt) is a process.
    -A process encompasses(keeps) the current status of the activity, called the process state.
    -process state includes
        -the current position in the program being executed (the value of the program counter)
        -other CPU registers and the associated memory cells
        - process state is a snapshot of the machine at that time. At different times during the execution of a program(at different times in a process) different snapshots (different process states) will be observed

    process table-
        -To keep track of the state of all the processes, the operating system maintains a table known as 
        the process table. 
        -Inside this table, every process is listed along with the resources the processes is using 
        and the current state of the process.

    PROCESS CONTROL BLOCK(PCB)
        -ds which stores the info about a particular process. this info is reqd by the cpu while executing the process
        -each process is identified by its own pcb, is also known as CONTEXT OF THE PROCESS
        -pcb of all process are present in the LINKED LIST
        -info stored in pcb are-
            1. process id (pid) :When a process is created, a unique id is assigned to the process 
                which is used for unique identification of the process in the system.
            2. program counter(pc): contains address of the next instruction to be executed
            3. process state : new/ready/running/waiting
            4. priority :
            5. general purpose registers (GPR): Every process has its own set of registers,
                which are used to hold the data which is generated during the execution of the process.
            6. list of open files: During the Execution, Every process uses some files which need to be present 
                in the main memory. OS also maintains a list of open files in the PCB
            7. list of open device: OS also maintain the list of all open devices which are used during 
                the execution of the process
            8. protections:
            *first 7 are important

    STATES OF PROCESS
        running state-
            the process has all the resources it need for execution and it has been given permission by 
             the operating system to use the processor.
            Only one process can be in the running state at any given time

        ready state-
            -waiting for permission to use the processor

        waiting state-
            -waiting for some external event to occur such as user input or a disk access

        #the waiting and ready states are implemented as QUEUES which hold the processes in these states

    degree of multiprogramming 
        -the no. of processes present in main memory at any point of time

    scheduler in os (long term scheduler, short term, mid term)
            1                   2                  3                4 
        - new state -(LTS)-> ready state -(STS)->running -(MTS)-> waiting {or waiting -(MTS)->running}
        -ie  1->2:lts,
            2->3: sts, (also kns as DISPATCHER) 
            3->4(or 4->2): mts - performs swapping (main memory <-> secondary memory)
        
        #DISPATCHER : responsible for saving the context of one process and loading the context of other process

SCHEDULING
    -The responsibility of determining how to allocate processor time among all the ready processes is known as scheduling

    PREEMPTIVE (running state <-> ready state)    
        -one approach to scheduling known as preemptive scheduling: 
         "this task is accomplished by dividing time into short segments, each called a time slice or quantum
         (typically about 50 milliseconds), 
        and then switching the CPU's attention among the processes as each is allowed to execute for no longer 
        than one time slice
        -context switching
            -This procedure of swapping processes is called a process switch or a context switch.
        
        #preemptive=> allows context switching b/w process in running state and that in ready state,
            even when process in runnig state has not finished its execution.

    NON PREEMPTIVE (ready state -> running state)
        - processes are give control of the processor until they complete execution or 
        voluntarily move themselves to a different state

    strategies for scheduling-

    First Come First Serve Scheduling (FCFS)
        This non-preemptive scheduling algorithm follows the first-in, first-out (FIFO) policy.
        As each process becomes ready, it joins the ready queue. When the current running process finishes execution, 
        the oldest process in the ready queue is selected to run next.
        ADV-
            -simple to use
            -easy to implement, can be implemented using QUEUE
            -does not lead to starvation
            #starvation
                -Starvation or indefinite blocking is phenomenon associated with the Priority scheduling algorithms, 
                in which a process ready to run for CPU can wait indefinitely because of low priority.
                
                *solution to starvation : AGING
                    -a technique of gradually increasing the priority of processes that wait in the system for a long time.
                *diff b/w deadlock and starvation
                    -When deadlock occurs no process can make progress, while in starvation apart from the victim process other processes can progress or proceed.
                

        DISADVANTAGE
            -doesnt consider priority or burst time (BT: time reqd by process for running on CPU,execution time)
            -suffers from CONVOY EFFECT (avg waiting time increases if job taking place first has large BT)

    Round Robin Scheduling
        This scheduling policy gives each process a slice of time (i.e., one quantum) before being preempted.
        As each process becomes ready, it joins the ready queue.
        A clock interrupt is generated at periodic intervals.
        When the interrupt occurs, the currently running process is preempted,
        and the *oldest process in the ready queue is selected to run next. 
        The time interval between each interrupt may vary.

    SHORTEST JOB FIRST 
        -preemptive 
            adv - gurantees MINIMUM AVG WAITING TIME
            disadv - not practical, as BT of process cant be knows before hand
        -non preemptive
        Shortest Process Next(srtf shortest remainig time first)
            This non-preemptive scheduling algorithm favors processes with the shortest expected execution time.
            As each process becomes ready, it joins the ready queue. When the current running process finishes 
            execution, the process in the ready queue with the shortest expected execution time is selected to run next.

    HRRN (Highest Response Ratio Next) Scheduling
        -the scheduling is done on the basis of an extra parameter called Response Ratio.
            A Response Ratio is calculated for each of the available jobs and 
            the Job with the highest response ratio is given priority over the others.
        - non-preemptive algorithm
        -one of the most optimal scheduling algorithms
        -its mode is non preemptive hence context switching is minimal in this algorithm

        #calculation of Response Ratio
        
            Response Ratio = (W+S)/S   
        where, W = waiting time, S = Service time or BT (burst time)
        eg. of HRRN: javatpoint.com/os-hrrn-example

        *can notice that: job with the shortest BT will be given priority but it is 
            also including an extra factor which is W (ie waiting time)
        

    PRIORITY SCHEDULING
        -there is a priority number assigned to each process
        -The priority number assigned to each of the process may or may not vary
        -Process with the higher priority among the available processes is given the CPU
        TYPES of priority scheduling
            1. PREEMPTIVE
            2. NON-PREEMPTIVE
            *The difference between preemptive priority scheduling and non preemptive priority scheduling is that, 
            in the preemptive priority scheduling, the job which is being executed can be stopped at the arrival of a higher priority job.
            *Once all the jobs get available in the ready queue, the algorithm will behave as non-preemptive priority scheduling, 
            which means the job scheduled will run till the completion and no preemption will be done.
        
        TYPES OF PRIORITY
            1. static - priority number doesn't change itself throughout the process, it is called static priority
            2. dynamic- priority no. keeps changing itself at the regular intervals
        

PURPOSE OF SCHEDULING
    -to increase CPU utilization
    -decrease turn around time (TAT =  CT - AT (completion time - arrival time))
    -minimize response time



THREADS VS PROCESS

THREADS
    -A thread is a single sequence stream within in a process
    -popular way to improve application through parallelism.
    - The CPU switches rapidly back and forth among the threads giving illusion that the threads are running in parallel.
    -Each thread has its own program counter (PC), a register set, and a stack space.
    -Threads are not independent of one other like processes
        as a result threads shares with other threads their code section, data section, OS resources  also known as 
        task, such as open files and signals.

    ADV - threads can share common data, they do not need to use interprocess communication.
        - context switching
        - sharing
    #process context switch-> more time consuming as there will be swithces b/w different pcb
    #threads
     switching time very less. can consider as a part of process, when we parallelism in job we break the job into different threads

    #implemention 
        i. user level
        ii. kernel level
            -> mapping is done b/w both then only it'll work 
            ->user level on its own has nt any existece-> it has to be mapped with kernel level 

Similarities
    -Like processes threads share CPU and only one thread active (running) at a time.
    -threads within a processes, threads within a processes execute sequentially.
    -Like processes, thread can create children.
    -And like process, if one thread is blocked, another thread can run.(???DISCUSS)
Differences
    -Unlike processes, threads are not independent of one another.
    -Unlike processes, all threads can access every address in the task .
    -Unlike processes, thread are design to assist one other. 
    Note that processes might or might not assist one another because processes may originate from different users


*benefits of multithreaded programming
    -It makes the system more responsive and enables resource sharing.
    It leads to the use of multiprocess architecture.
    It is more economical and preferred




*******

PROCESS SYNCHRONIZATION
    -process synchronization is reqd becz multiple processes are involved in a system and these processes access
    some shared resources
        *if they access (that shared resources) one by one then no problem
        *but if they try to access it in mixed fashion(ie simultaneously by context switching even when one process has not completed its work on resource)
            then system may give inconsistent result  ==> race condition
    #prblem: when two processes access the shared resource in the same time (ie not in proper orderly manner) -> this might lead to inconsistency k/as race around condition 
    #critical section: that section of code/ hardware/software which has been shared among different processes
    
    RACE CONDITION
        -different order of execution (of processes) result in different results when they share some resources or code(critical section), this cond is called race condition
        (as now diff process will race like the op produced by them is correct and not other processes)   

    eg. (refer lec-5.2 @sJan)
        #total no. of possible cases = n! / n1!*n2!*..  { divided by n1!*n2!*.. as order of execution of instructions within the process must be same(ie consider them as duplicates in arrangement)}
            where n = total number of instructions in all processes, 
            n1 = no of instructions in process1
            n2 = no of instructions in p2, ....


    CRITICAL SECTION PROBLEM: 
        -the problem arised due to critical section, when multiple process share critical sec at the same time
            system may go to inconsistent state

        #critical section is that part of the code where a process access sharable resources

        Following are three main crieterias to solve critical section problem! ->
        Mandatory Criteria:
            i. Mutual Exclusion: critical sec must be accessed in mutual exclusive way (ie one process access at a time)
            ii. Progress: consider only those process should compete to enter in critical sec those who want to (ie those process which doesnt 
                require to go in critical sec must not be unnecessary considered for critical sec problem)
        
        Optional Criteria:
            iii. Bounded Wait: There must be a max bound upto which a process can wait, ie after a specific time limit
                the process will be given chance to enter into the critical section

        SOLUTIONS: 

            ## 
                P{
                    entry section;
                    Critical Section;
                    Exit section;
                    remainder section;
                }

            sol1: solution of critical sec problem for two processes let p0, p1
                -coding in such a way that until one of the process is in cs other wont be able to access go into the cs

                #p0->p1 or p1->p0 by context switching (NOTE: at any pt of time context switching is possible)

                p0:                                                           p1: 
                    //turn = 0                                                  //turn = 1
                    while(1)                                                    while(1)
                    {                                                           {
                      
                        while(turn != 0)// false-> p0 will enter cs                 while(turn != 1)// false-> p1 will enter cs      
                      
                        Critical Section //p0 entering CS                           Critical Section //p1 entering CS
                      
                        turn = 1 //changing turn st next time p0                    turn = 0 //changing turn st next time p0  
                                 //will not enter cs                                          //will not enter cs

                        remainder section                                           remainder section
                    
                    }                                                           }

                #from above code it is clear that let say when p0 is in cs then no matter how many times p1 attempts
                to go in cs it will not be able to enter, as itll be stuck in infinite while loop until p0
                exits cs and changes the value of turn variable
                #ie above code follows 1st criteria : MUTUAL EXCLUSION
                
                #but it aint following 2nd criteria ie NO PROGRESS 
                    problem:
                        see above code is following strict alternation => ROUND ROBIN 
                        ie we are just giving access of cs to p0 and p1 alternately without even checking whether 
                        they want to go into cs or not, 
                        - now lets say p1 doesnt want to go into cs but p0 wants to but it wont be able to 
                        go into cs until p1 goes and then make p0's turn to go into cs 



                #=> sol1 is FAULTY as it doesnt satisfy Progress criteria, sol1 is suffering from strict alternation



            @sJan lec 5.5 
            sol2: (modifications done in sol1 so as to satisfy criteria 2 ie Progress)

                    p0:                                                         p1: 
                                        //flag[F,F]
                    while(1)                                                    while(1)
                    {                                                           {
                      
                        flag[0] = T;                                                flag[1] = T;
                        while(flag[1]);                                             while(flag[0]); //if flag[0]=t => p0 already in cs ie p1 will not be able to enter cs      
                      
                        Critical Section //p0 entering CS                           Critical Section //p1 entering CS
                      
                        flag[0] = F;                                                flag[1] = F;
                    }                                                           }

                #improvement: 
                    now we are not directly altering b/w process but instead checking wth of flag[] if a process wants to enter cs or not 
                #prob: deadlock 
                    => still NO PROGRESS 

            ///continue lec [5.6,  ]


@rooos
A solution to the critical section problem must satisfy the following three conditions:
    i) Mutual Exclusion. Out of a group of cooperating processes, only one process can be in its critical section at a given point of time.
    ii) Progress. ...
    iii) Bounded Waiting.

solution of critical section problem: i. Semaphores

    Semaphore-> Integer values 
        i)Critical section: 0, 1

        void wait(int &t) {
            while(t==0); //infinite loop(active wait->cpu is involved)
            t--;
        }

        void signal(int &t) {
            t++;
        }

    //order
        P->
        //initialize
        int t =1;

        //normal instructions
        //before entering critical section wait
        wait(t);
        //critical section
        //complete critical section instructions
        signal(t);

    eg:
    P1
        //normal instructions
        //before entering critical section wait
        wait(t);  //t=0
        //critical section
        //complete critical section instructions
        signal(t);

        //normal instructions

    P2
        //normal instructions
        //before entering critical section wait
        wait(t);
        //critical section
        //complete critical section instructions
        signal(t);

        //normal instructions


problems with semaphore:
    i. active wait
    ii. value of semaphore itself might be changed by the process
        sol for ii. 
            #monitor -> abstract data type used to solve cs problem
            -> similar to semaphore -> except -> the data types in semaphore are kept private and only funcions are public


    #problems solved with semaphores: diner philosopher problem, read write problem 






*********************************************************************
DEADLOCK
    -a situation in operating systems when there are two or more processes
    hold some resources and wait for resources held by other(s).

conditions
    -MUTUAL EXCLUSION:  there are resources that cannot be shared
    -HOLD AND WAIT: A process is holding at least one resource and waiting for another resource which is with some other process.
    -NO PREEMPTION: The operating system is not allowed to take a resource back from a process until process gives it back
    -CIRCULAR WAIT:A set of processes are waiting for each other in circular form


DIFF B/W STARVATION & DEADLOCK
1	Deadlock is a situation where no process got blocked and no process proceeds :	Starvation is a situation where the low priority process got blocked and the high priority processes proceed.
2	Deadlock is an infinite waiting :	Starvation is a long waiting but not infinite.
3	Every Deadlock is always a starvation :	Every starvation need not be deadlock.
4	The requested resource is blocked by the other process :	The requested resource is continuously be used by the higher priority processes.
5	Deadlock happens when Mutual exclusion, hold and wait : No preemption and circular wait occurs simultaneously.	It occurs due to the uncontrolled priority and resource management

*********************************************

@sJan (6.8-6.10 for paging numers)

            #2^10 = 1K, 2^20 = 1M, 2^30 = 1G, 2^40 = 1T(tera), 2^50 = 1P(peta), 2^60 = 1 zeta, 2^70 = 1 zebi
MEMORY MANAGEMENT

    goal is to achieve 
        i. memory size - larger
        ii. access time - lesser
        iii. per unit cost - lesser
        #but not all the above three can be obtained at with a single memory so to obtain this we've Hierarchy of memory

    Hierarchy of memory:
        CPU <-> CACHE MEMORY <-> MAIN MEMORY <-> SECONDARY MEMORY

                #for now we are considering -> CPU <-> MM <-> SM 
        Address Translation:
            cpu generates Logical Address (of SM), and to access MM we want Physical Address,so conversion of LA to PA is reqd

        How this hierarchy works->
            Locality of reference:
                --https://www.geeksforgeeks.org/locality-of-reference-and-cache-operation-in-cache-memory/  
                -Locality of Reference refers to the tendency of the computer program to access instructions 
                whose addresses are near one another
                -The property of locality of reference is mainly shown by loops and subroutine calls in a program.


        *task of os-> 
            i. memory allocation (ie how memo is to be allocated in mm and sm)
            ii. address Translation, 

        MEMORY ALLOCATION
            1. Contiguos Memory Allocation (eg array)
                -a process must be kept together completely in continuous order
                - memory allocation in sequential nature

                adv: 
                    i. Address Translation is easy (eg think of array: uk the base address and are supposed to find any address then u can simply reach to that addres by adding it with base address) 
                    ii. It is very fast ie Access Time will be less due to contiguos memo allocation to the process

                disadv: 
                    i. External Fragmentation: (cont. memo allocation always sufferes from external fragmentation)
                        When the space requested by a process is in total available but still cant be allocated
                        because the space is not available continuously and process wants space in Contiguos fashion.
                    ii. Internal Fragmentation:
                
                #External Fragmentation is severe issue compared to internal, as a lot of memory is wasted

            2. Non Contiguos Memory Allocation (eg linked list)
                -not necessary to keep entire process at one place, pointers are used to point where next instruction of the process is stored
                -access time is more as we need to access with the help of pointers and go one by one inorder to reach the reqd addres

                adv: 
                    i. Free From external fragmentation
                disadv:
                    i. slow - as access time will be more


            1. Contiguos Memory Allocation
                i. Fixed Size Partitioning
                    - Divide the memory into certain number of partitions and
                    - the size of each partion will be fixed and cannot be changed (size of diff partitions may differ from each other)
                    - Only one process can be accomodated in one partition. 
                    eg lets say we have partion of size 5 kb, 2 kb, 1 kb and a process requre 3kb space
                        then that process will be allocated 5kb space (as we cant divide and allocate 2 and 1 kb to the process due to contiguos allocation)
                         => 5-3 ie 2kb will be wasted from that partition -> this is k/as Internal Fragmentation

                    adv: 
                        i. Easy to  manage
                    disadv:
                        i. Internal Fragmentation
                            when a process can not use the entire space of the partion then the internal memory of the partition is wasted
                            (ie when size of partion is > size of process which has been allocated to that partion)
                        ii. External Fragmentation

                ii. Variable Size Partitioning
                    -to overcome the problems  in fixed size memo allocation
                    -No pre defined partition, whole system is treated as a single unit of memory and
                    as space is allocated acg to the requirement of the process in runtime

                    adv:
                        i. No internal Fragmentation
                            -As there are no fixed partitions => no memory waste due to internal remainig of partition which happened in fixed size partitioning
                    disadv: 
                        i. External Fragmentation

            SPACE ALLOCATION POLICIES (applicable in both fixed and variable size partitioning)
                1. First Fit
                    -starts searching (reqd space for the process)from the Start(of memory)
                    -allocate the first space available which is capable of accomodating the reqd process
                    #search starts from the Start of memory each time

                2. Best Fit
                    -searchs all the empty space or blocks available in the memory and
                    -allocates that block which is of smallest size and is capable of accomodating the process
                    #performs best in "Fixed size partitioning" as the remainig space of the block(partition) cannot be reused 

                3. Worst Fit
                    -largest block is allocated
                    => the remainig block space after allocating the process will also be large
                    => theres high probability that the remainig space can be reused later(considering var size partitioning)
                    
                    #Worst Fit -> performs best in Variable Size Partitioning

                    #Best Fit - performs best for Fixed Size Partitioning (as the remainig space in a block cant be reused anyways)
                              - performs worst for Variable Size Partitioning 
                            (because it uses the smallest block but the remainig space will be of very small size that 
                            theres very less possibility that it can be reused again)

                
                ADDRESS TRANSLATION     //@sJan 6.6
                    #cpu always generates Logical Address (ie for Secondary Memory)
                    #but cpu access MM not SM as accessing SM will be take more time
                    #so Physical Address is generated (to acces MM where the reqd info pointed by address generated by cpu is residing)

                    flow:
                        CPU -(la)-> LIMIT REGISTER -(traps if address is illegal, else go fwd) -> 
                        RELOCATION REGISTER -(adds base address of the process with instruction no => resulting pa)
                        -(pa)-> MAIN MEMORY
                            
                            *la = logical address, pa = physical address

                    Relocation Register: 
                        -holds the base address of the process in Main Memory
                    Limit Register:
                        -contains the size of the process and so->
                        -checks the address requested by cpu is legal or not ie 
                        the address is in limit (of the instruction number or say size of the process) of process or not


                Major Problem with Contiguos Memory Allocation:
                    External Fragmentation => fragmented space available but process demands contiguos space

                Sol: 
                    i. either we defragment the memory and make a contiguos space available for the process 
                        -which will be ofc very costly in runtime 
                    ii. we fragment the process and allocate it in non-contiguos fashion
                        => here comes Paging and Segmentation (ie non-contiguos memory allocation)

            2. Non-Contiguos Memory Allocation
                // we want to use MM more efficiently by removing external fragmentation-> concept of paging <-> as we are utilizing space we had to trade it with time as time will be doubled..
                Paging: 
                    -SM is divided into fixed size partition and size of each partition will be same
                    these partitions are called "Pages", and size of each page is same
                    -similarly MM is divided into fixed equal size partitions called "Frames"
                    *size of page = size of frame

                    Address Translation: 
                        cpu generates la = page no.(p) + instruction offset(d)
                        f = frame no.
                        
                        cpu -(p+d)-> page Table -(f+d)-> MM
                                                
                        *reqd page no will be searched in the page table of that process -> from there we'll get 
                            the address of frame where that page lies in MM -> then frame No. + instruction offset (=physical address)
                            -> and then we have physical addres finally to access MM

                    Page Table:
                        -data structure used as index to store the address of frames(in MM) of a particular process
                        -No. of entries in page table = no of pages in that process
                        -every process has its own page table independently
                        -pt stores the base addres(ie address of frame corresponding to the particular page)
                        of every page 

                    adv: 
                        -No External Fragmentation (memory utilization)
                    disadv:
                        -extra space reqd for page table in MM
                        -as it is Fixed Size Partitioning scheme => suffers from Internal Fragmentation
                        -instruction access time becomes DOUBLE -> as memory is accessed twice->
                            1. to access Page Table (inorder to convert la -> pa)
                            2. accessing mm (with generated pa) for required instruction

                        #as instruction access time becomes double => speed becomes half! --> this is a severe penalty


                    # n bit address => 2^n different addresses possible => memory size = (2^n)*size of location(generally 1 Byte)
                    # ie n bit address will support ((2^n)*size of location) size of memory

            Problem with Paging: 
                cpu generates logical address -> problem was MM was accessed twice:
                                                1. cuz of Page Table (which is stored in MM)
                                                2. to access the generated physical address (so as to get the reqd page's frame in MM)
                =>Access time double (page table + mm) => speed halved
            solution:
                TLB (Translational Look aside Buffer) : to resolve the timing penalty cuz of paging
            
            TLB 
                -stored in hardware (ie not in MM instead a special hardware used(not SM)) 
                -has very small size => access time will be lesser than MM(page table) access time
                -contains two columns -> i) Page No 
                                        ii) corresponding frame No.
                -Page is first searched in TLB when not found (ie that page is searched for the first time)
                    that page is searched in Page Table and then recorded into TLB so as in near fututre if again that 
                    page is searched than it can be found directly in TLB. 
                -Common TLB is shared among all the Processes, 
                -holds data of one process at a time
                =>when new process comes -> prev process's info needs to be flushed from the tlb

                    #avg memory access time using tlb: 
                        h(tlb + MM) +(1-h)*(tlb + MM + MM) //twice mm =>for page table + physical address 
                        where, h = tlb hit ratio, tlb = tlb access time, MM = main memory access time
                adv: 
                    avg memory access time reduced (as we dont have to access MM twice as soon as tlb gets filled, as in general tlb hit ratio>90%)
                disadv: 
                    -special hardware=> some additional cost
                    -performance of TLB degrades when theres multiple context switches(as hit ratio degrades due to multiple context switch)
                    //in case of multiple context switching -> lets say tlb just got filled and got speed but then
                        ->if theres context switch then entire page no details in tlb will be deleted as now tlb 
                        will be used to store the details for next process
                         
                Sol: 
                    tlb performance can be improved
                        i. either by using multiple copies of tlb
                        ii. by reserving some part of os so as when theres context swith the data of runnning process
                            is not deleted but saved in reserved memo and then when again the process starts executing 
                            its data will already be available in tlb

            
            Segmentation: 
            //https://www.geeksforgeeks.org/segmentation-in-operating-system/
                -Same as paging except instead of dividing memory into equal size pages, 
                division is done on the basis of different size of segments acg to process
                (lets say in a process we can have different segments for say main fun, add fun, etc etc)
                -size of segments need not necessary be same

                Segment table:
                    contains the base address of segment and its size -> 
                    so that the requested segment + instruction offset's range will be checked(with the help of ba and size of segment)
                   
                #Limit register used to check the validity of address requested by cpu
                
                Advantages of Segmentation –
                    No Internal fragmentation.
                    Segment Table consumes less space in comparison to Page table in paging.
                Disadvantage of Segmentation –
                    As processes are loaded and removed from the memory, 
                    the free memory space is broken into little pieces, causing External fragmentation




*********
@gfg 

CACHE MEMORY
    Cache memory is an extremely fast memory type that acts as a buffer between RAM and the CPU.
    It holds frequently requested data and instructions so that they are immediately available to the CPU when needed.
    Cache memory is used to reduce the average time to access data from the Main memory.
    There are various different independent caches in a CPU, which store instructions and data.
    
    //@L3.5+ (comp architecture @gateSmashers)
        CPU->cache(lines)->RAM(blocks)
        
        #size of lines in cache = size of blocks in RAM
        #here words => similar to byte (but 1 word =x bytes , x (- 1,2,.. )
        //consider similar to cpu->RAM(frames)->SM(pages)

    cache mapping types:
        i. Direct Mapping:              //https://www.youtube.com/watch?v=eObN3u3eAnU&list=PLxCzCOWd7aiHMonh3G6QNKq53C6oNXGrX&index=36
            -pos of blocks is directly fixed (k%n) in cache

            Ln = k%n 
            where k = block no, n = no of lines
                  Ln =Line no in which block k is to be placed in cache

            #pa = physical addres

            pa = block no. + block offset
            pa-> tag + line no. + block offset

            adv: fast search
            disadv: conflict miss more(ie frequently miss even when empty lines are available, cuz block pos are fixed for particular line(k%n))

        ii. Fully Assosiative mapping:
            -any block can be placed in any of the empty lines
            
            pa = block no. + block offset
            pa-> tag + block offset
            #here no need to fix line no as in any line any block can be placed 

            adv: cache hit more
            disad: comparison increased (for search)

        iii. K-way set associative Mapping:
            -direct+fully associative

            no of set(s) = no of lines / K 

            *k(blockNo.)%n =replaced=> k(blockNo)%s
            #k(blockNo)%s -> will give block k is in which set and then it can be any of the lines of that set

            pa=block no + block offset
            pa->cacheaddress=> tag + set no. + block offset


CACHE COHERENCE
// https://www.geeksforgeeks.org/cache-coherence/

***********************************************
VIRTUAL MEMORY



**********************************************
THRASHING
    Fall of CPU utilization with increase in Degree of multiprogramming

BELADY'S ANOMALY




